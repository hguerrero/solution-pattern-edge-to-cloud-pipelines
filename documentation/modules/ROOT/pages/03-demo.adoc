= Solution Pattern: Name Template
:sectnums:
:sectlinks:
:doctype: book
:imagesdir: ../assets/images

= See the Solution in Action

== Demonstration

The Demo primarily focusses on showcasing how the combination of key Red Hat technologies unlocks the capability of constructing a fully automated platform that chains the full cycle of acquiring data, training models, delivering and deploying models, and run live inferencing.

The sections below will help you walk through the essentials of installing and running the demo. But there is much more to discover and extract out of it, among other things:
  
  - Customise AI/ML models from publicly available pre-trained models.
  - Create developer-friendly AI/ML models (easy to integrate with).
  - Apply integration technology to move discrete or bulk-based training data.
  - Bridge remote services using Service Interconnect
  - Create scalable architectures growing numbers of edge environments.


[#demo-video]
=== Watch a demonstration

Coming soon!

== Run the demonstration

This Camel Quarkus component combines MQTT and HTTP clients (such as IoT devices, cellphones, and third-party clients) with an AI/ML engine to obtain image detection results.

=== Prerequisites

You will require:

- An _OpenShift Container Platform_ cluster running version 4.12 or above with cluster admin access and _Red Hat OpenShift AI_ installed.
- _Docker, Podman or Ansible_ installed and running. +
[NOTE]
  To run the demo's _Ansible Playbook_ to deploy it you'll need one of the above. If none of them is installed on your machine we suggest installing _Docker_ using the most recent Docker version. See the https://docs.docker.com/engine/installation/[Docker Engine installation documentation^] for further information.
+ 
[TIP]
  You'll find more information below on how to use _Podman_ or _Ansible_ as alternatives to _Docker_. 


{empty} +

### Provision an OpenShift environment

1. Provision the following _Red Hat Demo Platform_ (RHDP) item:
+
--
* https://demo.redhat.com/catalog?item=babylon-catalog-prod/community-content.com-edge-to-core.prod&utm_source=webapp&utm_medium=share-link[**Solution Pattern - Edge to Core Data Pipelines for AI/ML**^]

[NOTE]
  The provisioning of the RHDP card above will just prepare for you a base environment (OCP+AI). You still need to deploy the demo by running the installation process described below.

[NOTE]
  The provisioning process takes around 80-90 minutes to complete. You need to wait its completion before proceeding to the demo deployment.
--
+
{empty} +

1. Alternatively, if you don't have access to RHDP, ensure you have an _OpenShift_ environment available and install _Red Hat OpenShift AI_.
[TIP]
  You can obtain one by deploying the trial version available at https://www.redhat.com/en/technologies/cloud-computing/openshift/try-it[Try Red Hat OpenShift^].

{empty} +


=== Install the demo using _Docker_ or _Podman_

[TIP]
====
For more installation tips and alternative options to _Docker_ and _Podman_, look at the https://github.com/brunoNetId/sp-edge-to-cloud-data-pipelines-demo/blob/main/README.md[README^] file in the demo's GitHub repository.
====

Ensure your base _OpenShift+AI_ environment is ready and you have all the connection and credential details with you.

1. Clone this GitHub repository:
+
[.console-input]
[source,bash]
----
git clone https://github.com/brunoNetId/sp-edge-to-cloud-data-pipelines-demo.git
----

1. Change directory to the root of the project.
+
[.console-input]
[source,bash]
----
cd sp-edge-to-cloud-data-pipelines-demo
----

1. Configure the `KUBECONFIG` file to use (where kube details are set after login).
+
[.console-input]
[source,bash]
----
export KUBECONFIG=./ansible/kube-demo
----

1. Login into your OpenShift cluster from the `oc` command line.
+
[.console-input]
[source,bash]
----
oc login --username="admin" --server=https://(...):6443 --insecure-skip-tls-verify=true
----
+
Replace the `--server` url with your own cluster API endpoint.
+
{empty} +

1. Run the Playbook
+
* With Docker:
+
[.console-input]
[source,bash]
----
docker run -i -t --rm --entrypoint /usr/local/bin/ansible-playbook \
-v $PWD:/runner \
-v $PWD/ansible/kube-demo:/home/runner/.kube/config \
quay.io/agnosticd/ee-multicloud:v0.0.11  \
./ansible/install.yaml
----

* With Podman:
+
[.console-input]
[source,bash]
----
podman run -i -t --rm --entrypoint /usr/local/bin/ansible-playbook \
-v $PWD:/runner \
-v $PWD/ansible/kube-demo:/home/runner/.kube/config \
quay.io/agnosticd/ee-multicloud:v0.0.11  \
./ansible/install.yaml
----

{empty} +


=== Walkthrough guide

The guide below will help you to familiarise with the main components in the demo, and how to operate it to trigger the actions...

==== Quick Topology Overview

Open your _OpenShift_ console with your given admin credentials and open the _Topology View_ to inspect the main systems deployed in the _Edge1_ namespace.

Following the illustration below:

. Select from the left menu the Developer view
. Search in the filter textbox by `edge1`
. Select the project `edge1`
. Make sure you display the _Topologoy_ view (left menu)

image::12-topology-edge1.png[]

In the image above you'll see the main applications deployed in the _Edge_ zone:

- **Shopper**: This is the main AI-powered application. The application exposes a smart device App you can open from your phone or browser. The application integrates with the _AI/ML Model Server_ to request inferences, and also with the _Price Engine_ to obtain price information from the product catalogue.
+
The App has two main uses:
+
--
* Customers/Shoppers use it to obtain information about product, in this context of this demo, the price tag of a product.
* Staff members can generate training data by capturing images for new products.
+
{blank}
--
// +
// {empty} +

- **Model Server**: This is the AI/ML engine running inferences and capable of recognizing products. It exposes an API for clients to send an image, and responds with the product name identified. The Model Server is composed of:
  * TensorFlow model server: the AI/ML brain executor.
  * Minio instance (from where the models are loaded).
 
- **Price Engine**: This application keeps the product catalogue and contains the pricing information. It exposes an API to obtain product information where the price tag is included.

- **Manager**: This integration runs in the background monitoring the availability of new model versions in the Core Data Centre (_Central_). When a new model version is available it is responsible to obtain it and push it to the Model Server.

[NOTE]
You'll find in the _Edge_ project other systems also deployed, but we won't dive into them as they are of less importance to the main story. Some mentions will be done to them when the context is relevant. 

{empty} +

==== Play with the Smart Application

Let's interact with the _Edge_ environment from the Smart Application to see the system in action.

[IMPORTANT]
--
The model server has been preloaded with a first version of the model (**v1**), pre-trained to only recognise two types of tea:

_Earl Grey Tea_ and _Lemon Tea_.
image:14-tea-earl-grey.png[,10%]
image:15-tea-lemon.png[,10%]
--


First, let's run some negative tests by taking random pictures of objects around you. Because **v1** has not been trained to identify those objects, the system will not be able to provide a price for them and will respond with the label _"Other"_ (as in _'product not identified'_).

Open the _Shopper App_ by clicking on the _Route_ exposed by the application pod, as shown in the picture below:

image::13-open-shopper-app.png[,30%]

This action will open a new tab in your browser presenting the app's landing page.

[TIP]
You can also open the application from your smart phone if you share its URL to your device.

Next, follow the actions below illustrated to run some inferences. Observe the response on your screen every time you send an image.

image::16-detection-mode.jpg[]

[NOTE]
The App allows you to simulate an image transmittion via _HTTP_, as would tipically apps interact with backend servers, or via _MQTT_, a lightweight messaging protocol, commonly used in the _IoT_, preferable for edge devices constrained by network bandwidth, energy consumption and CPU power.

[NOTE]
In the demo, the App uses an _MQTT_ library that uses _Websockets_ to connect to the _AMQ Broker_ deployed in the _Edge_ project. The _Camel_ application connects via _MQTT_ to pick up the messages, process them and respond, also via _MQTT_.

You should see in your display the following response:

image:17-result-other.png[,30%, align=left]

It means it wasn't able to identify the object.

Let's now run some positive inferences. We have included in the GitHub repositories images that have been used to train the model. 

Make sure you operate from your computer's browser, and this time click on the `Pick from Device` button instead. This action will open your system's file picker.

To choose the images to test with, navigate to the following project path:

* `sp-edge-to-cloud-data-pipelines-demo/demo`

where you will find the following images:

* `tea-earl-grey.jpg`
* `tea-lemon.jpg`

Try them out. You should obtain positive results with the following responses:

[%autowidth]
|===
|_Earl Grey Tea: 3.99_
|===

[%autowidth]
|===
|_Lemon Tea: 4.99_
|===

{empty} +

==== Train a new product

The _Edge_ environment has been pre-loaded with training data. This will make it easy for you to produce a second version of the model (**v2**) which you can try out.

You can visualise the training data by opening _Minio_'s UI and browsing the `data` S3 bucket. Or you can use the following online S3 browser which will nicely display all the images to use for training, head to:

* https://www.filestash.app/s3-browser.html[Online S3 browser^] 

And enter the following details:

** Access Key ID: `minio`
** Secret Access ID: `minio123`
** Advanced >
*** Endpoint: [Minio's URL]

You can obtain your Minio instance URL by executing the following `oc` command:
[.console-input]
[source,bash]
----
oc get route minio-api -o custom-columns=HOST:.spec.host -n edge1
----

Your connection details on screen should look similar to the picture below:

image::18-s3-connect.png[,40%]

Click `CONNECT`, and select the folder (bucket) `data`.

Navigate to the folder `images/tea-green` where you should find all the training images you're about to use:

image::19-s3-data.jpg[,50%]

[NOTE]
This collection of training data was captured during a live demonstration where the audience participated in generating the images.

[TIP]
A quick reminder: **v1** does not know about this type of tea, it only knows about _Earl Grey Tea_ and _Lemon Tea_.


This new product is _"(Bali) Green Tea"_ and is labelled as `tea-green`. The price engine is also preconfigured with a specific price tag for this product.


We can trigger the training process from a hidden administrative page the _Shopper_ application includes. Use the following command to obtain the admin page URL address:

[.console-input]
[source,bash]
----
echo "https://`oc get route camel-edge -n edge1 --template={{.spec.host}}`/admin.html"
----

Copy the resulting URL address and use it in a new tab in your computer's browser.

A monitoring view will display all the playing parts in the demo. You will already be familiar with most of the parts shown on the monitoring view (which map with those visible from your _Topology_ view from the _OpenShift_'s console):

image::20-monitor-admin-view.jpg[]

Prior to initiating the training process, and now that you're familiar with the monitoring view, let's rewind a little and remind ourselves what processes are involved in the _Data Acquisition_ phase.

[NOTE]
We are bypassing the ingestion (_Data Acquisition_) phase to speed up the process of producing a second version of the model. Later you will participate in generating your own training data to produce a third version of the model.

The illustration below shows how, during the _Data Acquisition_ phase, training data is generated from devices and pushed to the system.

image::23-monitor-admin-ingestion.jpg[]

Each image, captured by a worker and sent over the network, is received and pushed to local S3 storage on the _Edge_. This phase may take a certain period of time until a large number of images is collected. To maximise accuracy you ideally want to train the model with vast amounts of training data.

To initiate the training process, click the button on the upper-left side of the window:

image:21-monitor-admin-button.jpg[,20%]

After you click `_Train Data_`, you'll see in the monitoring view a series of live animations illustrating the actions actually taking place in the platform. The following enumeration describes the process:

. The click action triggers a signal that a _Camel_ integration (_Manager_) picks up.
. The _Manager_ reads all the training data from the S3 bucket where it resides and packages it as a ZIP container.
. The _Manager_ invokes an API served from the Core Data Center (_Central_) to send the ZIP data.
. The system _Feeder_ (_Camel_) exposing the above requested API, unpacks the ZIP container and pushes the data to a central S3 service used as the storage system (_ODF_) for training new models.
. The same system _Feeder_ sends a signal via _Kafka_ to announce the arrival of new training data to be processed.
. The system _Delivery_ (Camel) is subscribed to the announcements topic. It receives the Kafka signal and triggers the Pipeline responsible the create the a new model version.
. The pipeline (_Tekton_) kicks off. It reads from the S3 storage system all the training data available and executes the Data Science notebooks based on _TensorFlow_
+
[NOTE]
The entire execution of the pipeline may take between 2-5 minutes depending on the resources allocated in the environment.
+
. At the end of the pipeline process, a new model is pushed to an edge-dedicated topic where new model placed.
. A copy of the new model version is also pushed to a Model repository. In this demo, just another S3 bucket, where a history of model versions is kept.

All the steps above form part of the _Data Preparation and Modelling_ phase (described in the _Architecture_ chapter) and are well illustrated in the diagram below:

image::22-monitor-admin-pipeline.jpg[]

But the process has not finished yet. It then enters into the _Delivery_ phase. The new model has now been pushed to an S3 bucket `edge1-ready` that is being monitored by an integration point on the Edge (_Manager_)

[TIP]
_Edge_ and _Core_ are connected via _Service Interconnect_. Both regions are running an instance of _Skupper_ to form virtual services which securely interconnect systems from both sides.

When the _Tekton_ pipeline uploads the new model to the S3 bucket, the _Edge Manager_ notices the artifacts and initiates the download of the model and hot deploys it in the TensorFlow model server, as shown in the picture below:

image::24-monitor-admin-delivery.jpg[]

The AI/ML engine, powered by the _TensorFlow Model Server_, reacts to the new version (**v2**), now available in its local S3 bucket, and initiates a hot-deployment. It loads the new version and discards the old one that was held in memory. This process happens without service interruption. Clients sending inference requests inadvertently start obtaining results computed with the new hot-deployed version (**v2**).

