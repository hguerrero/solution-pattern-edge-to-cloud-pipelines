= Solution Pattern: Name Template
:sectnums:
:sectlinks:
:doctype: book
:imagesdir: ../assets/images

= See the Solution in Action

== Demonstration

The Demo primarily focusses on showcasing how the combination of key Red Hat technologies unlocks the capability of constructing a fully automated platform that chains the full cycle of acquiring data, training models, delivering and deploying models, and run live inferencing.

The sections below will help you walk through the essentials of installing and running the demo. But there is much more to discover and extract out of it, among other things:
  
  - Customise AI/ML models from publicly available pre-trained models.
  - Create developer-friendly AI/ML models (easy to integrate with).
  - Apply integration technology to move discrete or bulk-based training data.
  - Bridge remote services using Service Interconnect
  - Create scalable architectures growing numbers of edge environments.


[#demo-video]
=== Watch a demonstration

Coming soon!

== Install the demonstration

This Camel Quarkus component combines MQTT and HTTP clients (such as IoT devices, cellphones, and third-party clients) with an AI/ML engine to obtain image detection results.

=== Prerequisites

You will require:

- An _OpenShift Container Platform_ cluster running version 4.12 or above with cluster admin access and _Red Hat OpenShift AI_ installed.
- _Docker, Podman or Ansible_ installed and running. +
[NOTE]
  To run the demo's _Ansible Playbook_ to deploy it you'll need one of the above. If none of them is installed on your machine we suggest installing _Docker_ using the most recent Docker version. See the https://docs.docker.com/engine/installation/[Docker Engine installation documentation^] for further information.
+ 
[TIP]
  You'll find more information below on how to use _Podman_ or _Ansible_ as alternatives to _Docker_. 


{empty} +

### Provision an OpenShift environment

1. Provision the following _Red Hat Demo Platform_ (RHDP) item:
+
--
* https://demo.redhat.com/catalog?item=babylon-catalog-prod/community-content.com-edge-to-core.prod&utm_source=webapp&utm_medium=share-link[**Solution Pattern - Edge to Core Data Pipelines for AI/ML**^]

[NOTE]
  The provisioning of the RHDP card above will just prepare for you a base environment (OCP+AI). You still need to deploy the demo by running the installation process described below.

[NOTE]
  The provisioning process takes around 80-90 minutes to complete. You need to wait its completion before proceeding to the demo deployment.
--
+
{empty} +

1. Alternatively, if you don't have access to RHDP, ensure you have an _OpenShift_ environment available and install _Red Hat OpenShift AI_.
[TIP]
  You can obtain one by deploying the trial version available at https://www.redhat.com/en/technologies/cloud-computing/openshift/try-it[Try Red Hat OpenShift^].

{empty} +


=== Install the demo using _Docker_ or _Podman_

[TIP]
====
For more installation tips and alternative options to _Docker_ and _Podman_, look at the https://github.com/brunoNetId/sp-edge-to-cloud-data-pipelines-demo/blob/main/README.md[README^] file in the demo's GitHub repository.
====

Ensure your base _OpenShift+AI_ environment is ready and you have all the connection and credential details with you.

1. Clone this GitHub repository:
+
[.console-input]
[source,bash]
----
git clone https://github.com/brunoNetId/sp-edge-to-cloud-data-pipelines-demo.git
----

1. Change directory to the root of the project.
+
[.console-input]
[source,bash]
----
cd sp-edge-to-cloud-data-pipelines-demo
----

1. Configure the `KUBECONFIG` file to use (where kube details are set after login).
+
[.console-input]
[source,bash]
----
export KUBECONFIG=./ansible/kube-demo
----

1. Login into your OpenShift cluster from the `oc` command line.
+
[.console-input]
[source,bash]
----
oc login --username="admin" --server=https://(...):6443 --insecure-skip-tls-verify=true
----
+
Replace the `--server` url with your own cluster API endpoint.
+
{empty} +

1. Run the Playbook
+
* With Docker:
+
[.console-input]
[source,bash]
----
docker run -i -t --rm --entrypoint /usr/local/bin/ansible-playbook \
-v $PWD:/runner \
-v $PWD/ansible/kube-demo:/home/runner/.kube/config \
quay.io/agnosticd/ee-multicloud:v0.0.11  \
./ansible/install.yaml
----

* With Podman:
+
[.console-input]
[source,bash]
----
podman run -i -t --rm --entrypoint /usr/local/bin/ansible-playbook \
-v $PWD:/runner \
-v $PWD/ansible/kube-demo:/home/runner/.kube/config \
quay.io/agnosticd/ee-multicloud:v0.0.11  \
./ansible/install.yaml
----

{empty} +


== Walkthrough guide

The guide below will help you to familiarise with the main components in the demo, and how to operate it to trigger the actions...

=== Quick Topology Overview

Open your _OpenShift_ console with your given admin credentials and open the _Topology View_ to inspect the main systems deployed in the _Edge1_ namespace.

Following the illustration below:

. Select from the left menu the Developer view
. Search in the filter textbox by `edge1`
. Select the project `edge1`
. Make sure you display the _Topologoy_ view (left menu)

image::12-topology-edge1.png[]

In the image above you'll see the main applications deployed in the _Edge_ zone:

- **Shopper**: This is the main AI-powered application. The application exposes a smart device App you can open from your phone or browser. The application integrates with the _AI/ML Model Server_ to request inferences, and also with the _Price Engine_ to obtain price information from the product catalogue.
+
The App has two main uses:
+
--
* Customers/Shoppers use it to obtain information about product, in this context of this demo, the price tag of a product.
* Staff members can generate training data by capturing images for new products.
+
{blank}
--
// +
// {empty} +

- **Model Server**: This is the AI/ML engine running inferences and capable of recognizing products. It exposes an API for clients to send an image, and responds with the product name identified. The Model Server is composed of:
  * TensorFlow model server: the AI/ML brain executor.
  * Minio instance (from where the models are loaded).
 
- **Price Engine**: This application keeps the product catalogue and contains the pricing information. It exposes an API to obtain product information where the price tag is included.

- **Manager**: This integration runs in the background monitoring the availability of new model versions in the Core Data Centre (_Central_). When a new model version is available it is responsible to obtain it and push it to the Model Server.

[NOTE]
You'll find in the _Edge_ project other systems also deployed, but we won't dive into them as they are of less importance to the main story. Some mentions will be done to them when the context is relevant. 

{empty} +

=== Play with the Smart Application

Let's interact with the _Edge_ environment from the Smart Application to see the system in action.

[IMPORTANT]
--
The model server has been preloaded with a first version of the model (**v1**), pre-trained to only recognise two types of tea:

_Earl Grey Tea_ and _Lemon Tea_.
image:14-tea-earl-grey.png[,10%]
image:15-tea-lemon.png[,10%]
--


First, let's run some negative tests by taking random pictures of objects around you. Because **v1** has not been trained to identify those objects, the system will not be able to provide a price for them and will respond with the label _"Other"_ (as in _'product not identified'_).

Open the _Shopper App_ by clicking on the _Route_ exposed by the application pod, as shown in the picture below:

image::13-open-shopper-app.png[,30%]

This action will open a new tab in your browser presenting the app's landing page.

[TIP]
You can also open the application from your smart phone if you share its URL to your device.

Next, follow the actions below illustrated to run some inferences. Observe the response on your screen every time you send an image.

image::16-detection-mode.jpg[]

[NOTE]
The App allows you to simulate an image transmittion via _HTTP_, as would tipically apps interact with backend servers, or via _MQTT_, a lightweight messaging protocol, commonly used in the _IoT_, preferable for edge devices constrained by network bandwidth, energy consumption and CPU power.

[NOTE]
In the demo, the App uses an _MQTT_ library that uses _Websockets_ to connect to the _AMQ Broker_ deployed in the _Edge_ project. The _Camel_ application connects via _MQTT_ to pick up the messages, process them and respond, also via _MQTT_.

You should see in your display the following response:

image:17-result-other.png[,20%, align=left]

It means it wasn't able to identify the object.

Let's now run some positive inferences. We have included in the GitHub repository images that have been used as input to train the model. 

Make sure you operate from your computer's browser, and this time click on the button `Pick from Device` instead. This action will open your system's file chooser.

To pick the images to test with, navigate to the following project path:

* `sp-edge-to-cloud-data-pipelines-demo/demo`

where you will find the following images:

* `tea-earl-grey.jpg`
* `tea-lemon.jpg`

Try them out. You should obtain positive results with the following responses:

[%autowidth]
|===
|_Earl Grey Tea: 3.99_
|===

[%autowidth]
|===
|_Lemon Tea: 4.99_
|===

{empty} +

In the section that follows you will train a new version of the model (**v2**) to include a third type of tea, _Bali Green Tea_, which **v1** does not identify.

Before you continue to the next section, run one last negative to confirm the model does not know about it.

. Enter the _Detection Mode_ in your Smart App
. Click on the `Pick from Device` button.
. Navigate to the following project path:
+
--
* `sp-edge-to-cloud-data-pipelines-demo/demo`

where you will find the image:

* `tea-bali.jpg`
--
+
. Select and send it via HTTP or MQTT

You should obtain the negative response:

image:17-result-other.png[,20%, align=left]


{empty} +

=== Train a new product

The _Edge_ environment has been pre-loaded with training data. This will make it easy for you to produce a second version of the model (**v2**) which you can try out.

You can visualise the training data by opening _Minio_'s UI and browsing the `data` S3 bucket. Or you can use the following online S3 browser which nicely displays all the images to use for training, head to:

* https://www.filestash.app/s3-browser.html[Online S3 browser^] 

And enter the following details:

** Access Key ID: `minio`
** Secret Access ID: `minio123`
** Advanced >
*** Endpoint: [Minio's URL]

You can obtain your Minio instance URL by executing the following `oc` command:
[.console-input]
[source,bash]
----
oc get route minio-api -o custom-columns=HOST:.spec.host -n edge1
----

Your connection details on screen should look similar to the picture below:

image::18-s3-connect.png[,40%]

Click `CONNECT`, and select the folder (bucket) `data`.

Navigate to the folder `images/tea-green` where you should find all the training images you're about to use:

image::19-s3-data.jpg[,50%]

[NOTE]
This collection of training data was captured during a live demonstration where the audience participated in generating the images.

[TIP]
A quick reminder: **v1** does not know about this type of tea, it only knows about _Earl Grey Tea_ and _Lemon Tea_.


This new product is _"(Bali) Green Tea"_ and is labelled as `tea-green`. The price engine is also preconfigured with a specific price tag for this product.


We can trigger the training process from a hidden administrative page the _Shopper_ application includes. Use the following command to obtain the admin page URL address:

[.console-input]
[source,bash]
----
echo "https://`oc get route camel-edge -n edge1 --template={{.spec.host}}`/admin.html"
----

Copy the resulting URL address and use it in a new tab in your computer's browser.

A monitoring view will display all the playing parts in the demo. You will already be familiar with most of the parts shown on the monitoring view (which map to those visible from your _Topology_ view from the _OpenShift_'s console):

image::20-monitor-admin-view.jpg[]

{empty} +

==== Review the _Data Acquisition_ phase

Prior to initiating the training process, and now that you're familiar with the monitoring view, let's rewind a little and remind ourselves what processes are involved in the _Data Acquisition_ phase.

[NOTE]
We are bypassing the ingestion (_Data Acquisition_) phase to speed up the process of producing a second version of the model. Later you will participate in generating your own training data to produce a third version of the model.

The illustration below shows how, during the _Data Acquisition_ phase, training data is generated from devices and pushed to the system.

image::23-monitor-admin-ingestion.jpg[]

Each image, captured by a worker and sent over the network, is received and pushed to local S3 storage on the _Edge_. This phase may take a certain period of time until a large number of images is collected. To maximise accuracy you ideally want to train the model with vast amounts of training data.

{empty} +

==== Enter the _Training_ phase

To initiate the training process, click the button on the upper-left side of the window:

image:21-monitor-admin-button.jpg[,20%]

After you click `_Train Data_`, you'll see in the monitoring view a series of live animations illustrating the actions actually taking place in the platform. The following enumeration describes the process:

. The click action triggers a signal that a _Camel_ integration (_Manager_) picks up.
. The _Manager_ reads all the training data from the S3 bucket where it resides and packages it as a ZIP container.
. The _Manager_ invokes an API served from the Core Data Center (_Central_) to send the ZIP data.
+
[TIP]
_Edge_ and _Core_ are connected via _Service Interconnect_. Both regions are running an instance of _Skupper_ to form virtual services which securely interconnect systems from both sides.
+
. The system _Feeder_ (_Camel_) exposing the above requested API, unpacks the ZIP container and pushes the data to a central S3 service used as the storage system (_ODF_) for training new models.
. The same system _Feeder_ sends a signal via _Kafka_ to announce the arrival of new training data to be processed.
. The system _Delivery_ (Camel) is subscribed to the announcements topic. It receives the Kafka signal and triggers the Pipeline responsible the create the a new model version.
. The pipeline (_Tekton_) kicks off. It reads from the S3 storage system all the training data available and executes the Data Science notebooks based on _TensorFlow_
+
[NOTE]
The entire execution of the pipeline may take between 2-5 minutes depending on the resources allocated in the environment.
+
. At the end of the pipeline process, a new model is pushed to an edge-dedicated topic where new model placed.
. A copy of the new model version is also pushed to a Model repository. In this demo, just another S3 bucket, where a history of model versions is kept.

All the steps above form part of the _Data Preparation and Modelling_ phase (described in the _Architecture_ chapter) and are well illustrated in the diagram below:

image::22-monitor-admin-pipeline.jpg[]

{empty} +

==== The _Delivery_ phase

The end-to-end process is not done yet. It then enters into the _Delivery_ phase. The new model has now been pushed to an S3 bucket `edge1-ready` that is being monitored by an integration point on the Edge (_Manager_)

[TIP]
_Edge_ and _Core_ are connected via _Service Interconnect_. Both regions are running an instance of _Skupper_ to form virtual services which securely interconnect systems from both sides.

When the _Tekton_ pipeline uploads the new model to the S3 bucket, the _Edge Manager_ notices the artifacts and initiates the download of the model and hot deploys it in the TensorFlow model server, as shown in the picture below:

image::24-monitor-admin-delivery.jpg[]

The AI/ML engine, powered by the _TensorFlow Model Server_, reacts to the new version (**v2**), now available in its local S3 bucket, and initiates a hot-deployment. It loads the new version and discards the old one that was held in memory. This process happens without service interruption. Clients sending inference requests inadvertently start obtaining results computed with the new hot-deployed version (**v2**).

{empty} +

==== The _Inferencing_ phase

The platform keeps running its live services at all times. Customers (shoppers) and workers interact with the platform while, in the background, new models are continuously being trained, delivered and deployed.

The demo's inferencing phase is illustrated in the picture below:

image::25-monitor-admin-inferencing.jpg[]

You should already be familiar with the flows above. You had the chance to perform some positive/negative tests via HTTP/MQTT. The _Shopper_ application (_Camel_) first sends an inference request against the AI/ML engine to identify the product, then:

* If the AI engine identifies the product, it provides a label, for example `tea-lemon`, and _Camel_ calls the Price Engine to obtain a price tag for that product. The image is also kept in S3 storage as it may be used to improve the accuracy of future models.
* If the AI engine does not identify the product (negative response `other`), Camel directly pushes the image to an S3 bucket of unidentified images. This may help Data Scientists to analyse the data.

You've already interacted with the application using the demo App. Let's use it again to try out the newly trained version.

[WARNING]
--
Before continuing, make sure your pipeline has finished execution. You can use your _OpenShift_'s console to inspect the state of the _PipelineRun_, or you can execute the following `oc` command to monitor it:

[.console-input]
[source,bash]
----
oc get pipelinerun -n tf
----

When the pipeline completes successfully, you should see the following output:

----
NAME                    SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME
train-model-run-ljrdk   True        Succeeded   92m         89m
----
--

Go back to the Smart Application in your browser and this time, with the newly trained model (**v2**), send the _Bali Green Tea_ image that **v1** didn't know about.

. Enter the _Detection Mode_ in your Smart App
. Click on the `Pick from Device` button.
. Navigate to the following project path:
+
--
* `sp-edge-to-cloud-data-pipelines-demo/demo`

where you will find the image:

* `tea-bali.jpg`
--
+
. Select and send it via HTTP or MQTT

This time, the product should be identified, and you should obtain a price tag as follows:

[%autowidth]
|===
|_(Bali) Green Tea: 2.49_
|===

Bravo !! you have completed the full cycle.

{empty} +


=== Create your own product

Up until now, you've played with pre-configured systems, and pre-loaded data, to train **v2**. It is time to go one level up. You will configure the system to create a new entry in the product catalogue, generate training data for the new product, train the new model **v3**, and run live inferences against it.


==== Configure the _Price Engine_

First things first, head to your OpenShift console and find the following _ConfigMap_:

* `catalogue` +
This configmap is owned by the _Price Engine_ and configures all the products available.

Edit the catalogue to include a new product. +
Follow the steps below to find your way:

image:26-configure-configmap.jpg[]

. From the console (`edge1` project), click on the menu option `ConfigMaps`
. From the list of displayed configmaps, select `catalogue`.
. You'll find the option to `Edit ConfigMap` from the top right-right corner of the console.
* Click Actions -> Edit ConfigMap
. Locate the lower-right corner of the text area
. Click and drag the corner downwards to expand the text area.

The JSON data configures the products. You'll find in the definition all the products you have been playing with.

Now include in the configuration a new product.
If one does not come to mind, use the JSON data below to configure a `Computer Mouse` product:

[.console-input]
[source,json]
----
    {
      "item": "computer-mouse",
      "label": "Computer Mouse",
      "price": 19.99
    },
----

Copy the JSON node above and paste it in the _ConfigMap_.

[WARNING]
Make sure your JSON document is valid after finishing editing. Make sure commas (`,`) are in the right place.

Click `Save`.

Now you need to restart the _Price Engine_. You can simply kill the pod and Kubernetes will restart a new one that will read the new ConfigMap value.

You can use the web Console to do so, or execute the `oc` command below:

[.console-input]
[source,bash]
----
oc delete pod -n edge1 `oc get pods -n edge1 | grep price | awk '{print $1}'`
----

You can validate, by inspecting the logs, the new started pod has loaded the new product catalogue. Again, you can use the web console, or execute from your terminal the command below:

[.console-input]
[source,bash]
----
oc logs -n edge1 `oc get pods -n edge1 | grep price | awk '{print $1}'`
----

In the output logs from the command above, you should find the value `"Computer Mouse"`. Your logs should look similar to:

[source,bash]
----
. . .
2024-03-29 19:58:41,544 INFO  [pri.xml:74] (Camel (camel-1) thread #1 - timer://products) ["Earl Grey Tea", "(Bali) Green Tea", "Lemon Tea", "Computer Mouse", "Other"]
----

Next, you need to configure the Smart Application to allow selecting the new product for training.

{empty} +

==== Configure the _Shopper_ application

You need to perform a similar operation. From your OpenShift console, find the following _ConfigMap_:

* `shopper-training-options` +
This ConfigMap is owned by the _Shopper_ (_Camel_) integration and configures all the trainable products from the App's '_Ingestion Mode_' (Data Acquisition).

Follow the same steps as previously done to find the ConfigMap and open the Edit window.

. From the console (`edge1` project), click on the menu option `ConfigMaps`
. From the list of displayed configmaps, select `shopper-training-options`.
. You'll find the option to `Edit ConfigMap` from the top right-right corner of the console.
* Click Actions -> Edit ConfigMap
. Locate the lower-right corner of the text area
. Click and drag the corner downwards to expand the text area.

Replace the old product by the new one. +
If you used the previous sample configuration for the Computer Mouse, copy the value below and replace the old product in your text area:

[.console-input]
[source,json]
----
[
  {
      "item": "computer-mouse",
      "label": "Computer Mouse"
  }
]
----

[TIP]
You can also add a sequence of procuts. Instead of deleting the old product, you can add the new one separated with a comma.

[WARNING]
Make sure your JSON document is valid after finishing editing. Make sure commas (`,`) are in the right place. +
Also, the values in this configuration need to match those in the product catalogue. +

[NOTE]
This information configures display options in the Smart App. Notice you don't define a price tag here.

Click `Save`.

Now you need to restart the _Shopper_. You can simply kill the pod and Kubernetes will restart a new one that will read the new ConfigMap value.

You can use the Web Console to do so, or execute the `oc` command below:

[.console-input]
[source,bash]
----
oc delete pod -n edge1 `oc get pods -n edge1 | grep shopper | awk '{print $1}'`
----