= Solution Pattern: Name Template
:sectnums:
:sectlinks:
:doctype: book
:imagesdir: ../assets/images

= See the Solution in Action

== Demonstration

The Demo primarily focusses on showcasing how the combination of key Red Hat technologies unlocks the capability of constructing a fully automated platform that chains the full cycle of acquiring data, training models, delivering and deploying models, and run live inferencing.

The sections below will help you walk through the essentials of installing and running the demo. But there is much more to discover and extract out of it, among other things:
  
  - Customise AI/ML models from publicly available pre-trained models.
  - Create developer-friendly AI/ML models (easy to integrate with).
  - Apply integration technology to move discrete or bulk-based training data.
  - Bridge remote services using Service Interconnect
  - Create scalable architectures growing numbers of edge environments.


[#demo-video]
=== Watch a demonstration

Coming soon!

== Run the demonstration

This Camel Quarkus component combines MQTT and HTTP clients (such as IoT devices, cellphones, and third-party clients) with an AI/ML engine to obtain image detection results.

=== Prerequisites

You will require:

- An _OpenShift Container Platform_ cluster running version 4.12 or above with cluster admin access and _Red Hat OpenShift AI_ installed.
- _Docker, Podman or Ansible_ installed and running. +
[NOTE]
  To run the demo's _Ansible Playbook_ to deploy it you'll need one of the above. If none of them is installed on your machine we suggest installing _Docker_ using the most recent Docker version. See the https://docs.docker.com/engine/installation/[Docker Engine installation documentation^] for further information.
+ 
[TIP]
  You'll find more information below on how to use _Podman_ or _Ansible_ as alternatives to _Docker_. 


{empty} +

### Provision an OpenShift environment

1. Provision the following _Red Hat Demo Platform_ (RHDP) item:
+
--
* https://demo.redhat.com/catalog?item=babylon-catalog-prod/community-content.com-edge-to-core.prod&utm_source=webapp&utm_medium=share-link[**Solution Pattern - Edge to Core Data Pipelines for AI/ML**^]

[NOTE]
  The provisioning of the RHDP card above will just prepare for you a base environment (OCP+AI). You still need to deploy the demo by running the installation process described below.

[NOTE]
  The provisioning process takes around 80-90 minutes to complete. You need to wait its completion before proceeding to the demo deployment.
--
+
{empty} +

1. Alternatively, if you don't have access to RHDP, ensure you have an _OpenShift_ environment available and install _Red Hat OpenShift AI_.
[TIP]
  You can obtain one by deploying the trial version available at https://www.redhat.com/en/technologies/cloud-computing/openshift/try-it[Try Red Hat OpenShift^].

{empty} +


=== Install the demo using _Docker_ or _Podman_

[TIP]
====
For more installation tips and alternative options to _Docker_ and _Podman_, look at the https://github.com/brunoNetId/sp-edge-to-cloud-data-pipelines-demo/blob/main/README.md[README^] file in the demo's GitHub repository.
====

Ensure your base _OpenShift+AI_ environment is ready and you have all the connection and credential details with you.

1. Clone this GitHub repository:
+
[.console-input]
[source,bash]
----
git clone https://github.com/brunoNetId/sp-edge-to-cloud-data-pipelines-demo.git
----

1. Change directory to the root of the project.
+
[.console-input]
[source,bash]
----
cd sp-edge-to-cloud-data-pipelines-demo
----

1. Configure the `KUBECONFIG` file to use (where kube details are set after login).
+
[.console-input]
[source,bash]
----
export KUBECONFIG=./ansible/kube-demo
----

1. Login into your OpenShift cluster from the `oc` command line.
+
[.console-input]
[source,bash]
----
oc login --username="admin" --server=https://(...):6443 --insecure-skip-tls-verify=true
----
+
Replace the `--server` url with your own cluster API endpoint.
+
{empty} +

1. Run the Playbook
+
* With Docker:
+
[.console-input]
[source,bash]
----
docker run -i -t --rm --entrypoint /usr/local/bin/ansible-playbook \
-v $PWD:/runner \
-v $PWD/ansible/kube-demo:/home/runner/.kube/config \
quay.io/agnosticd/ee-multicloud:v0.0.11  \
./ansible/install.yaml
----

* With Podman:
+
[.console-input]
[source,bash]
----
podman run -i -t --rm --entrypoint /usr/local/bin/ansible-playbook \
-v $PWD:/runner \
-v $PWD/ansible/kube-demo:/home/runner/.kube/config \
quay.io/agnosticd/ee-multicloud:v0.0.11  \
./ansible/install.yaml
----

{empty} +


=== Walkthrough guide

The guide below will help you to familiarise with the main components in the demo, and how to operate it to trigger the actions...

==== Quick Topology Overview

Open your _OpenShift_ console with your given admin credentials and open the _Topology View_ to inspect the main systems deployed in the _Edge1_ namespace.

Following the illustration below:

. Select from the left menu the Developer view
. Search in the filter textbox by `edge1`
. Select the project `edge1`
. Make sure you display the _Topologoy_ view (left menu)

image::12-topology-edge1.png[]

In the image above you'll see the main applications deployed in the _Edge_ zone:

- **Shopper**: This is the main AI-powered application. The application exposes a smart device App you can open from your phone or browser. The application integrates with the _AI/ML Model Server_ to request inferences, and also with the _Price Engine_ to obtain price information from the product catalogue.
+
The App has two main uses:
+
--
* Customers/Shoppers use it to obtain information about product, in this context of this demo, the price tag of a product.
* Staff members can generate training data by capturing images for new products.
+
{blank}
--
// +
// {empty} +

- **Model Server**: This is the AI/ML engine running inferences and capable of recognizing products. It exposes an API for clients to send an image, and responds with the product name identified. The Model Server is composed of:
  * TensorFlow model server: the AI/ML brain executor.
  * Minio instance (from where the models are loaded).
 
- **Price Engine**: This application keeps the product catalogue and contains the pricing information. It exposes an API to obtain product information where the price tag is included.

- **Manager**: This integration runs in the background monitoring the availability of new model versions in the Core Data Centre (_Central_). When a new model version is available it is responsible to obtain it and push it to the Model Server.

[NOTE]
You'll find in the _Edge_ project other systems also deployed, but we won't dive into them as they are of less importance to the main story. Some mentions will be done to them when the context is relevant. 

{empty} +

==== Play with the Smart Application

Let's interact with the _Edge_ environment from the Smart Application to see the system in action.

[IMPORTANT]
--
The model server has been preloaded with a first version of the model (**v1**), pre-trained to only recognise two types of tea:

_Earl Grey Tea_ and _Lemon Tea_.
image:14-tea-earl-grey.png[,10%]
image:15-tea-lemon.png[,10%]
--


First, let's run some negative tests by taking random pictures of objects around you. Because **v1** has not been trained to identify those objects, the system will not be able to provide a price for them and will respond with the label _"Other"_ (as in _'product not identified'_).

Open the _Shopper App_ by clicking on the _Route_ exposed by the application pod, as shown in the picture below:

image::13-open-shopper-app.png[,30%]

This action will open a new tab in your browser presenting the app's landing page.

[TIP]
You can also open the application from your smart phone if you share its URL to your device.

Next, follow the actions below illustrated to run some inferences. Observe the response on your screen every time you send an image.

image::16-detection-mode.jpg[]

[NOTE]
The App allows you to simulate an image transmittion via _HTTP_, as would tipically apps interact with backend servers, or via _MQTT_, a lightweight messaging protocol, commonly used in the _IoT_, preferable for edge devices constrained by network bandwidth, energy consumption and CPU power.

[NOTE]
In the demo, the App uses an _MQTT_ library that uses _Websockets_ to connect to the _AMQ Broker_ deployed in the _Edge_ project. The _Camel_ application connects via _MQTT_ to pick up the messages, process them and respond, also via _MQTT_.

You should see in your display the following response:

image::17-result-other.png[,20%]

It means it wasn't able to identify the object.

Let's now run some positive inferences. We have included in the GitHub repositories images that have been used to train the model. 

Make sure you operate from your computer's browser, and this time click on the `Pick from Device` button instead. This action will open your system's file picker.

To choose the images to test with, navigate to the following project path:

* `sp-edge-to-cloud-data-pipelines-demo/demo`

where you will find the following images:

* `tea-earl-grey.jpg`
* `tea-lemon.jpg`

Try them out. You should obtain positive results with the following responses:

[%autowidth]
|===
|_Earl Grey Tea: 3.99_
|===

[%autowidth]
|===
|_Lemon Tea: 4.99_
|===

{empty} +

==== Train a new product