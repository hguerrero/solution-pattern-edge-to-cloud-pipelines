= Solution Pattern: Name Template
:sectnums:
:sectlinks:
:doctype: book

= Architecture 

Introduction for the architecture of this solution pattern.

== Common Challenges 

To better explain and detail the reasons for the existence of this solution pattern we’ll picture some common needs and challenges amongst organizations that already have production systems and seeks innovation and modernization.

AI/ML production deployment is an iterative process that goes beyond simply generating AI/ML models or even ML Ops.

The primary milestones in the AI/ML life cycle depending on an organization's business goals for its AI/ML effort are as follows:

- Collect and prepare the data needed for your AI/ML project. It is critical to be astute in terms of what functions take place at the edge. We do not want to do compute or storage intensive operations at the Edge, but rather should use the core or cloud.
What we should concentrate on at the edge is data collecting and data inference, where you will use AIML to make real-time decisions.
- Create ML models based on business objectives. MLOps should take place on the core or in the cloud to retrain models as needed.
- Integrate the ML models with the intelligent applications that you will create to serve the model and assist in making critical business choices.
- Model accuracy should be monitored and managed throughout time.

.AI/ML lifecycle for the Edge.

image::02-AIML-lifecycle.png[]

As organizations continue to grapple with massive amounts of data being generated from sources ranging from device edge to off-site facilities and public and private cloud environments, data managers are encountering a new challenge: how to properly ingest and process that data to receive actionable intelligence in a timely manner.

With fresh, relevant data, businesses can learn effectively and adapt to changing customer behavior. However, managing vast amounts of ingested data  and preparing to make that data ready as soon as possible—preferably in real time—for analytics and artificial intelligence and machine learning (AI/ML), is extremely challenging for data engineers.

[#tech_stack]
== Technology Stack

Red Hat® OpenShift® Container Platform, Red Hat OpenShift Data Foundation, Red Hat Application Foundations, and other tools can help automate the workflow of data ingestion, preparation, and management building data pipelines for hybrid cloud deployments that automate data processing upon acquisition.

=== Red Hat Technology

// Change links and text here as you see fit.
* https://www.redhat.com/en/technologies/device-edge[Red Hat Device Edge]
* https://www.redhat.com/en/products/application-foundations[Red Hat Application Foundations,window=_blank]
** *https://access.redhat.com/products/red-hat-amq#broker[AMQ Broker,window=_blank]:* Pure-Java multi-protocol message broker. It’s built on an efficient, asynchronous core with a fast native journal for message persistence and the option of shared-nothing state replication for high availability.
** *https://access.redhat.com/products/red-hat-amq#streams[AMQ Streams,window=_blank]:* Based on the Apache Kafka project, AMQ Streams offers a distributed backbone that allows microservices and other applications to share data with extremely high throughput and extremely low latency.
** *https://access.redhat.com/documentation/en-us/red_hat_build_of_apache_camel_extensions_for_quarkus/2.13/html/getting_started_with_camel_extensions_for_quarkus/index[Red Hat build of Apache Camel Extensions for Quarkus,window=_blank]:* Utilize the integration capabilities of Apache Camel and its vast component library in the Quarkus runtime, optimizing for peak application performance with fast start up time.
** *https://access.redhat.com/products/quarkus[Red Hat build of Quarkus,window=_blank]:* Utilizes an innovative compile-time boot process that moves typical runtime steps to compile time. The result is an application that can consume as little as 10’s of MB of memory and start in 10’s of milliseconds.
* https://www.redhat.com/en/technologies/cloud-computing/openshift[Red Hat OpenShift,window=_blank]
* https://www.redhat.com/es/technologies/cloud-computing/openshift/openshift-data-science[Red Hat OpenShift Data Science]

=== Additional Technology:

** https://www.postgresql.org/[PostgreSQL database,window=_blank]
** https://helm.sh/[Helm,window=_blank]


[#in_depth]
== An in-depth look at the solution's architecture

Ingesting and preparing data can help organizations deploy automated data pipelines in an event-driven architecture. Continuous data, such as sensor metrics, temperature, and vibrations can be imported directly into an Apache Kafka topic in AMQ Streams.

=== Data Acquisition

Ingesting data into a distributed architecture allows data engineers to create pipelines that can help automate workflows and business processes.

- *Discrete data:* Images, video, and records are input as objects into an object bucket. 
- *Multiprotocol:* Data can be acquire using different protocols. Databases can connect to Kafka through a Debezium connector.

Data can be enhanced for an enriched data discovery experience, and it can be modified at various stages of the data pipeline life cycle as required by the use case.

- *Enhancement:* Objects can be enhanced with multifaceted metadata to make data discovery easier for search, analytics, and AI/ML workloads.
- *Modification:* Objects can be modified by AI tools. For example, an image may be anonymized to obscure sensitive data.

The first stage begins with image acquisition. The check-out kiosks capture every item that the cashier scans. Each image is then sent over MQTT to the AMQ Broker-implemented *Event Broker*. The data is then passed on to the *Image Processor* processing application, which prepares it for transmission. The framework provides an easy way to connect to the broker and generate a lightweight native build or container image, so this application is generated using Red Hat build or Apache Camel Extensions for Quarkus.

The image is cleaned up by the processing application to remove sensitive or private information and resized for management purposes. The resulting data is saved in the *Software Defined Storage* filesystem, and the metadata is routed to a Kafka topic in the AMQ Streams *Event Bus*, where streaming applications built with the Kafka Streams Quarkus extension or Camel can perform additional parallel processing while benefiting from the platform's streaming and repeatability capabilities.

Mirror Maker 2 technology is used to transport metadata from the edge to the core. The platform offers replication possibilities for saved data. When connected to the core, both components support information replication.

[TIP]
If the edge constraints prevent the deployment of software-defined storage, we may still save the files locally and then run a Camel pipeline that implements the logic to replicate the data to the core using conventional protocols such as S3.

See below an abstract representation of the stage:

.Data Acquisition Overview.

image::02-data-acquisition.png[]


=== Data Preparation and Modeling

One way to manage the flow of objects into the data pipeline is to create notifications that initiate a workflow. 

- *Object bucket:* Object bucket notifications can be pushed to various endpoints, such as Kafka, HTML, or Advanced Message Queuing Protocol (AMQP).
- *Notifications:* As objects enter, exit, or are modified in the bucket, a bucket notification is made to a Kafka broker in Red Hat AMQ.
Change data capture: Red Hat Integration tools initiate processes like data replication and microservices integration through CDC.

=== Application Development and Delivery

Data moves through the pipeline and can trigger events. As the data is analyzed, it may trigger additional events creating an automated event-driven workflow.

=== Edge ML Inference

Data moves through the pipeline and can trigger events. As the data is analyzed, it may trigger additional events creating an automated event-driven workflow.

- *Eventing:* Whether discrete data has been pushed to a Kafka topic or continuous data is sent directly to the Kafka broker, the Kafka producer will call a service that writes to a Kafka topic and initiates an event. 
- *Elasticity:* Red Hat OpenShift Serverless can receive these event triggers and spawn multiple applications such as inferencing, alerts, messaging, anonymization, and preventative remediation. 
- *Edge to core:* Kafka can also mirror data from edge locations to a core repository for further processing.
Life cycle: Prioritized data can be moved back into a data repository for ML retraining, constituting a continuous improvement pipeline.


[#more_tech]
== About the Technology Stack

If you want to include more details about the tech stack you used, this is the place.

// end::arch-in-depth[]