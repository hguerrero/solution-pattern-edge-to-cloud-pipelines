= Solution Pattern: Edge to Core Data Pipelines for AI/ML
:sectnums:
:sectlinks:
:doctype: book
:imagesdir: ../assets/images

= Architecture 

Introduction for the architecture of this solution pattern.

== Common Challenges 

To better explain and detail the reasons for the existence of this solution pattern we’ll picture some common needs and challenges amongst organizations that already have production systems and seeks innovation and modernization.

AI/ML production deployment is an iterative process that goes beyond simply generating AI/ML models or even ML Ops.

The primary milestones in the AI/ML life cycle depending on an organization's business goals for its AI/ML effort are as follows:

- Collect and prepare the data needed for your AI/ML project. It is critical to be astute in terms of what functions take place at the edge. We do not want to do compute or storage intensive operations at the Edge, but rather should use the core or cloud.
What we should concentrate on at the edge is data collecting and data inference, where you will use AIML to make real-time decisions.
- Create ML models based on business objectives. MLOps should take place on the core or in the cloud to retrain models as needed.
- Integrate the ML models with the intelligent applications that you will create to serve the model and assist in making critical business choices.
- Model accuracy should be monitored and managed throughout time.

.AI/ML lifecycle for the Edge.

image::02-AIML-lifecycle.png[,80%]

As organizations continue to grapple with massive amounts of data being generated from sources ranging from device edge to off-site facilities and public and private cloud environments, data managers are encountering a new challenge: how to properly ingest and process that data to receive actionable intelligence in a timely manner.

With fresh, relevant data, businesses can learn effectively and adapt to changing customer behavior. However, managing vast amounts of ingested data  and preparing to make that data ready as soon as possible—preferably in real time—for analytics and artificial intelligence and machine learning (AI/ML), is extremely challenging for data engineers.

[#tech_stack]
== Technology Stack

Red Hat® OpenShift® Container Platform, Red Hat OpenShift Data Foundation, Red Hat Application Foundations, and other tools can help automate the workflow of data ingestion, preparation, and management building data pipelines for hybrid cloud deployments that automate data processing upon acquisition.

// === Red Hat Technology

// Change links and text here as you see fit.
* https://www.redhat.com/en/technologies/device-edge[Red Hat Device Edge]
* https://www.redhat.com/en/products/application-foundations[Red Hat Application Foundations,window=_blank]
** *https://access.redhat.com/products/red-hat-amq#broker[AMQ Broker,window=_blank]:* Pure-Java multi-protocol message broker. It’s built on an efficient, asynchronous core with a fast native journal for message persistence and the option of shared-nothing state replication for high availability.
** *https://access.redhat.com/products/red-hat-amq#streams[AMQ Streams,window=_blank]:* Based on the Apache Kafka project, AMQ Streams offers a distributed backbone that allows microservices and other applications to share data with extremely high throughput and extremely low latency.
** *https://developers.redhat.com/products/redhat-build-of-apache-camel[Red Hat build of Apache Camel,window=_blank]:* Utilize the integration capabilities of Apache Camel and its vast component library in the Quarkus runtime, optimizing for peak application performance with fast start up time.
** *https://access.redhat.com/products/quarkus[Red Hat build of Quarkus,window=_blank]:* Utilizes an innovative compile-time boot process that moves typical runtime steps to compile time. The result is an application that can consume as little as 10’s of MB of memory and start in 10’s of milliseconds.
* https://www.redhat.com/en/technologies/cloud-computing/openshift[Red Hat OpenShift,window=_blank]
* https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-ai[Red Hat OpenShift AI]
* https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation[Red Hat OpenShift Data Foundation]
* https://www.redhat.com/en/technologies/cloud-computing/service-interconnect[Red Hat Service Interconnect]

// === Additional Technology:

// ** https://www.postgresql.org/[PostgreSQL database,window=_blank]
// ** https://helm.sh/[Helm,window=_blank]


[#in_depth]
== An in-depth look at the solution's architecture

This solution pattern implements an architecture that allows large organisations to have a core data center doing all the CPU-hungry processing (AI/ML training) where different branches of the company are connected to push training data and obtain upgraded model versions.

In the retail space for instance you may find a company that runs multiple stores with different needs. One branch (_Edge1_ for example) runs a big surface area store, with a big product catalogue aimed at the every-day citizen. Another branch (_Edge2_), small-sized store, may be located in a prominent area and offers luxury products to their customers. Both require different product catalogues and run at different speeds.


image::07-full-architecture.png[,80%]

Each _Near Edge_ zone in the diagram above represents the branches (stores) and run independent from each other. They all rely on the main data center to produce iterations of the AI/ML models they run for their stores.

[TIP]
====
The demo provided in this Solution Pattern can be provisioned with multiple edge zones, as per the diagram above, to showcase zone isolation and scalability (growing number of branches).
====

In the sections that follow the focus is centred in the interactions between one of near-edge zones (one single branch) and the core data center.


=== Data Acquisition

Ingesting and preparing data help organizations to deploy automated data pipelines in an event-driven architecture. In other spaces where continuous data is produced, such as sensor metrics, temperature, and vibrations can be imported directly into an Apache Kafka topic in AMQ Streams for example.

Data engineers can create pipelines to help automate workflows and business processes.

- *Discrete data:* Images, video, documents, are types of content data you can store in object buckets. 
- *Multiprotocol:* Data can be acquired using different protocols. Databases can connect to Kafka through a Debezium connector for example.

The _Data Acquisition_ stage in the solution pattern is presented in the diagram below:

image::08-stage1-data-aquisition.png[]

When a new product needs to be added in the catalogue, its imagery is pushed to the system (_Device_ to _Edge1_). The data is kept in local object storage while waiting to complete the collection. When the imagery is collected in full, an integration application (_Camel_) packages all the data and sends it over the wire to a remote S3 bucket located in the main data center (_Central_).

When the training data upload to S3 completes, a coordination signal is sent via Kafka to announce new training data is available, allowing

[NOTE]
====
Edge/Core connectivity is enabled with Service Interconnect. More details to follow.
====

{empty} +

=== Data Preparation & Modelling

The second stage is focussed on two important tasks:

- Design and Implement the AI/ML model that meets the business requirements.
- Prepare/Pre-process the raw data to be used as input training data.

There is considerable manual labour involved into the activities above listed. However, once the team of data scientists produce the desired implementation to generate the AI/ML models, it can be injected as part of an automated pipeline, along with other automated steps.

The same is true for the pre-processing work to refine the raw training data which can be added to the chain of automated steps in the pipeline.

The solution pattern implements this stage as a fully automated workflow, initiated by the Kafka signal sent by the near edge system as illustrated in the diagram below:

image::09-stage2-data-prep-n-model.png[]

The Kafka (_Streams_) event (signal) announces new training data is available. The _Camel_ integration consumes the event and triggers the automated pipeline.

The pipeline obtains all the training data from the S3 bucket and starts training the new AI/ML model. When the training completes, it uploads the new version to a model repository.

=== Application Development and Delivery

The third stage is primarily focussed on including all the https://www.redhat.com/en/topics/devops[_DevOps_] and https://www.redhat.com/en/topics/ai/what-is-mlops[_MLOps_] processes necessary to deliver the applications and upgrades the near edge needs to deploy and run.

Here, all the best cloud-native practices apply. To know more, follow the link of the resources listed below:

- https://developers.redhat.com/e-books/devops-culture-and-practice-openshift[DevOps Culture and Practice with OpenShift]
- https://www.redhat.com/en/resources/mlops-architecture-openshift-infographic[MLOps: Machine learning operations with Red Hat OpenShift]


Smart applications, powered by the AI/ML models, need to be designed and implemented along with their automated delivery mechanisms.

The demo provided in this _Solution Pattern_ already includes all the applications and integration systems. The focus is set on showcasing the automated pipeline responsible for producing new model versions which are uploaded to the platform's object storage system, keeping duplicates in a model repository.

The diagram below illustrates the stage:

image::10-stage3-app-dev-delivery.png[]

When the pipeline produces new model versions it pushes a copy into an edge-dedicated S3 bucket. An integration system (Camel) on the near edge environment monitors the bucket and when a new model version is available it downloads and hot-deploys it in the model server.

=== Edge ML Inference

The last stage involves customers and users generating live traffic that interacts with the platform and triggers AI/ML inferences against the Model server.

Other systems are also at play in orchestrated workflows, executing business use cases and delivering responses to customers.

image::11-stage4-ml-inferencing.png[]

The demo also provides a monitoring view that provides more in-depth insight into the systems that are involved end-to-end.

[#more_tech]
== Additional notes about Edge Computing

[NOTE]
Although not explicitly implemented in the Solution Pattern's demonstration, the following sections discusses topics very relevant to our _Solution Pattern_.

https://www.redhat.com/en/topics/edge-computing/what-is-edge-computing[Edge computing] shifts computing power away from core data-centers and distributes it closer to users and data sources—often across a large number of locations, providing faster response times, more reliable services, and a better application experience back to users.


=== What is Red Hat Device Edge?

Red Hat® Device Edge extends operational consistency across edge and hybrid cloud environments, no matter where devices are deployed in the field. Red Hat Device Edge combines enterprise-ready lightweight Kubernetes container orchestrations using MicroShift with Red Hat Enterprise Linux® to support different use cases and workloads on small, resource-constrained devices at the farthest edge.

MicroShift comes as an RPM software package that you can add to the blueprint of your system images when needed. Include your Kubernetes workloads, too, if you want. They will be deployed the next time you roll out updates to your devices. Red Hat Device Edge with MicroShift runs on Intel and Arm systems as small as 2 CPU cores and 2GB RAM.

MicroShift also provides OpenShift’s APIs for security context constraints and routes, but to reduce footprint we’ve removed APIs that are only useful on build clusters or clusters with multi-user interactive access. We’ve also removed Operators responsible for managing the operating system updates and configuration or orchestrating control plane components, as they are not needed in the MicroShift model.

.Red Hat Device Edge Technical Overview.

image::02-device-edge.png[,60%]

[TIP]
Learn more about Red Hat Device Edge collaborations with https://www.redhat.com/en/about/press-releases/lockheed-martin-red-hat-collaborate-advance-artificial-intelligence-military-missions[Lockheed Martin] and https://www.redhat.com/en/about/press-releases/abb-and-red-hat-partner-deliver-further-scalable-digital-solutions-across-industrial-edge-and-hybrid-cloud[ABB].

{empty} +

=== Single Node Apache Kafka Broker

The Red Hat® AMQ streams component is a massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. It offers a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. 

The latest AMQ Streams release introduces the new `UseKRaft` feature gate. This feature gate provides a way to deploy a Kafka cluster in the KRaft (Kafka Raft metadata) mode without ZooKeeper. This feature gate is currently in an experimental stage, but it can be used for development and testing of AMQ Streams and Apache Kafka.

.KRaft architecture for Kafka..

image::02-kafka-kraft-cluster.png[,60%]

****
With KRaft, we can deploy a single node Kafka broker that also serves as the controller. All of the advantages of stream processing in a small footprint.
****

// end::arch-in-depth[]